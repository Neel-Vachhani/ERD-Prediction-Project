{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5bbd45",
   "metadata": {},
   "source": [
    "# Stage 2 â€“ Approach 3 (Graph2Vec + Text Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41bdaa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelvachhani/Downloads/for_students/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/neelvachhani/Downloads/for_students/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, math, re, hashlib, networkx as nx, numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e091558",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMEL = re.compile(r\"(?<!^)(?=[A-Z])\")\n",
    "NON_ALNUM = re.compile(r\"[^a-z0-9]+\")\n",
    "SYN = {\"id\":\"identifier\",\"ids\":\"identifier\",\"num\":\"number\",\"no\":\"number\",\"pk\":\"primarykey\",\"fk\":\"foreignkey\"}\n",
    "\n",
    "normalize = lambda t: \" \".join(\n",
    "    SYN.get(p, p)\n",
    "    for p in NON_ALNUM.sub(\n",
    "        \" \",\n",
    "        CAMEL.sub(\n",
    "            \" \",\n",
    "            t.replace(\"_\", \" \").replace(\"-\", \" \"),\n",
    "        ).lower(),\n",
    "    ).strip().split()\n",
    "    if p\n",
    ")\n",
    "\n",
    "def bow(e):\n",
    "    p = [f\"erd_id:{e['id']}\", f\"dataset:{e['ds'][1]}\"]\n",
    "    for x in e.get(\"ents\", []):\n",
    "        p += [f\"entity:{normalize(x.get('name',''))}\", f\"entity_kind:{normalize(x.get('kind',''))}\"]\n",
    "        p += [f\"attr:{normalize(a)}\" for a in x.get(\"attributes\", [])]\n",
    "        p += [f\"pk:{normalize(a)}\" for a in x.get(\"primary_keys\", [])]\n",
    "    for r in e.get(\"rels\", []):\n",
    "        p += [f\"rel:{normalize(r.get('name',''))}\", f\"rel_kind:{normalize(r.get('kind',''))}\"]\n",
    "        p += [f\"relattr:{normalize(a)}\" for a in r.get(\"attributes\", [])]\n",
    "        for s in r.get(\"involved_entities\", []):\n",
    "            card = str(s.get(\"cardinality\", \"unknown\")).lower()\n",
    "            nm = normalize(s.get(\"name\", \"\"))\n",
    "            p += [f\"relside:{nm}:{card}\", f\"relcard:{card}\"]\n",
    "    flags = (e.get(\"comment\") or {}).get(\"flags\", []) if isinstance(e.get(\"comment\"), dict) else []\n",
    "    for f in flags:\n",
    "        for k, v in [(\"code\", \"comment_code\"), (\"target_type\", \"comment_target_type\"), (\"target_name\", \"comment_target_name\")]:\n",
    "            vv = normalize(str(f.get(k, \"\")))\n",
    "            if vv:\n",
    "                p.append(f\"{v}:{vv}\")\n",
    "    return \" \".join(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e50cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_erd(path, suffix):\n",
    "    d = json.loads(path.read_text())\n",
    "    stem = Path(path).stem\n",
    "    num = int(stem.split(\"_\")[0])\n",
    "    return {\"id\": str(num), \"ds\": (num, suffix), \"ents\": d.get(\"entities\", []), \"rels\": d.get(\"relationships\", []), \"comment\": d.get(\"comment\")}\n",
    "\n",
    "def load_all_erds(root):\n",
    "    out = {}\n",
    "    for ds_dir in [root/\"Dataset1\", root/\"Dataset2\"]:\n",
    "        suf = 1 if ds_dir.name.endswith(\"1\") else 2\n",
    "        for fp in sorted(ds_dir.glob(\"*.json\")):\n",
    "            e = _load_erd(fp, suf)\n",
    "            out[e[\"ds\"]] = e\n",
    "    return out\n",
    "\n",
    "def load_for_testing(root):\n",
    "    out = []\n",
    "    for ds_dir in [root/\"for_testing\"/\"Dataset1\", root/\"for_testing\"/\"Dataset2\"]:\n",
    "        suf = 1 if ds_dir.name.endswith(\"1\") else 2\n",
    "        for fp in sorted(ds_dir.glob(\"*.json\")):\n",
    "            out.append(_load_erd(fp, suf))\n",
    "    return out\n",
    "\n",
    "def load_grades(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    g = {}\n",
    "    for _, r in df.iterrows():\n",
    "        g1 = float(r[\"dataset1_grade\"]) if not math.isnan(r[\"dataset1_grade\"]) else None\n",
    "        g2 = float(r[\"dataset2_grade\"]) if not math.isnan(r[\"dataset2_grade\"]) else None\n",
    "        g[int(r[\"ERD_No\"])] = (g1, g2)\n",
    "    return g\n",
    "\n",
    "def build_train_lists(erds, grades):\n",
    "    train_erds = []\n",
    "    train_labels = []\n",
    "    for (n, ds), e in sorted(erds.items()):\n",
    "        gp = grades.get(n)\n",
    "        if gp and gp[ds-1] is not None:\n",
    "            train_erds.append(e)\n",
    "            train_labels.append(float(gp[ds-1]))\n",
    "    return train_erds, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844e933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_erd_graph(e):\n",
    "    g = nx.Graph()\n",
    "    rid = 0\n",
    "    for en in e.get(\"ents\", []):\n",
    "        enm = en.get(\"name\", \"\")\n",
    "        enode = f\"ent::{enm}\"\n",
    "        g.add_node(enode, label=\"entity\")\n",
    "        for a in en.get(\"attributes\", []):\n",
    "            an = f\"att::{enm}::{a}\"\n",
    "            g.add_node(an, label=\"attr\")\n",
    "            g.add_edge(enode, an)\n",
    "        for pk in en.get(\"primary_keys\", []):\n",
    "            pn = f\"pk::{enm}::{pk}\"\n",
    "            g.add_node(pn, label=\"pk\")\n",
    "            g.add_edge(enode, pn)\n",
    "    for r in e.get(\"rels\", []):\n",
    "        rid += 1\n",
    "        rn = f\"rel::{rid}\"\n",
    "        g.add_node(rn, label=\"rel\")\n",
    "        for a in r.get(\"attributes\", []):\n",
    "            ra = f\"ratt::{rid}::{a}\"\n",
    "            g.add_node(ra, label=\"rel_attr\")\n",
    "            g.add_edge(rn, ra)\n",
    "        for s in r.get(\"involved_entities\", []):\n",
    "            card = s.get(\"cardinality\", \"unknown\") or \"unknown\"\n",
    "            cn = f\"card::{rid}::{s.get('name','')}::{card}\"\n",
    "            g.add_node(cn, label=f\"card::{card}\")\n",
    "            g.add_edge(rn, cn)\n",
    "            g.add_edge(cn, f\"ent::{s.get('name','')}\")\n",
    "    if not g.nodes:\n",
    "        g.add_node(\"empty\", label=\"empty\")\n",
    "    return g\n",
    "\n",
    "class Graph2VecEmbedder:\n",
    "    def __init__(self, dimensions=192, wl_iterations=2, seed=42):\n",
    "        self.dim = dimensions\n",
    "        self.wl = wl_iterations\n",
    "        self.seed = seed\n",
    "    def _wl_feats(self, g):\n",
    "        if not g.nodes:\n",
    "            return {\"empty\": 1}\n",
    "        labels = {n: g.nodes[n].get(\"label\", \"unk\") for n in g.nodes}\n",
    "        bags = []\n",
    "        for _ in range(self.wl + 1):\n",
    "            bags.extend(labels.values())\n",
    "            labels = {n: hashlib.md5((str(self.seed) + labels[n] + \"|\" + \"|\".join(sorted(labels.get(nb, \"\") for nb in g.neighbors(n)))).encode()).hexdigest() for n in g.nodes}\n",
    "        return dict(Counter(bags))\n",
    "    def _fit_matrix(self, erds):\n",
    "        feats = [self._wl_feats(build_erd_graph(e)) for e in erds]\n",
    "        vocab = sorted(set().union(*feats)) or [\"empty\"]\n",
    "        idx = {k: i for i, k in enumerate(vocab)}\n",
    "        m = np.zeros((len(feats), len(vocab)))\n",
    "        for r, f in enumerate(feats):\n",
    "            for k, v in f.items():\n",
    "                m[r, idx[k]] = v\n",
    "        return np.log1p(m), vocab, idx\n",
    "    def _transform_matrix(self, erds):\n",
    "        m = np.zeros((len(erds), len(self.idx)))\n",
    "        for r, e in enumerate(erds):\n",
    "            f = self._wl_feats(build_erd_graph(e))\n",
    "            for k, v in f.items():\n",
    "                j = self.idx.get(k)\n",
    "                if j is not None:\n",
    "                    m[r, j] = v\n",
    "        return np.log1p(m)\n",
    "    def fit(self, erds):\n",
    "        if not erds:\n",
    "            self.train_emb = np.zeros((0, self.dim))\n",
    "            self.idx = {}\n",
    "            return self.train_emb\n",
    "        mat, vocab, idx = self._fit_matrix(erds)\n",
    "        self.idx = idx\n",
    "        self.vocab = vocab\n",
    "        n = max(1, min(self.dim, mat.shape[0], mat.shape[1] or 1))\n",
    "        self.svd = TruncatedSVD(n_components=n, random_state=self.seed)\n",
    "        self.train_emb = self._pad(self.svd.fit_transform(mat))\n",
    "        return self.train_emb\n",
    "    def transform(self, erds):\n",
    "        if not erds:\n",
    "            return np.zeros((0, self.dim))\n",
    "        if not hasattr(self, 'idx') or not self.idx:\n",
    "            return np.zeros((len(erds), self.dim))\n",
    "        mat = self._transform_matrix(erds)\n",
    "        red = self.svd.transform(mat) if hasattr(self, 'svd') and mat.shape[1] > 0 else np.zeros((len(erds), min(self.dim, mat.shape[1] or 1)))\n",
    "        return self._pad(red)\n",
    "    def fit_transform_pair(self, train_erds, extra_erds):\n",
    "        tr = self.fit(train_erds)\n",
    "        ex = self.transform(extra_erds)\n",
    "        return tr, ex\n",
    "    def _pad(self, x):\n",
    "        return x[:, :self.dim] if x.shape[1] >= self.dim else np.hstack([x, np.zeros((x.shape[0], self.dim - x.shape[1]))])\n",
    "\n",
    "class TextFeatures:\n",
    "    def __init__(self, name=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "        self.m = SentenceTransformer(name)\n",
    "    def fit(self, erds):\n",
    "        if erds:\n",
    "            self.m.encode([bow(erds[0])], show_progress_bar=False)\n",
    "    def transform(self, erds):\n",
    "        return self.m.encode([bow(e) for e in erds], show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4e963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedKNN:\n",
    "    def __init__(self, k=9, min_similarity=0.05):\n",
    "        self.k = k\n",
    "        self.q = min_similarity\n",
    "    def fit(self, v, y):\n",
    "        self.v = np.atleast_2d(v)\n",
    "        self.y = np.array(y, float)\n",
    "        self.mean = float(self.y.mean())\n",
    "        self.vn = self.v / (np.linalg.norm(self.v, axis=1, keepdims=True) + 1e-12)\n",
    "    def predict_one(self, vec):\n",
    "        vn = vec / (np.linalg.norm(vec) + 1e-12)\n",
    "        s = self.vn @ vn\n",
    "        elig = np.where(s >= self.q)[0]\n",
    "        if elig.size == 0:\n",
    "            return self.mean\n",
    "        top = elig[s[elig].argsort()[::-1][:self.k]]\n",
    "        w = s[top]\n",
    "        if np.allclose(w.sum(), 0):\n",
    "            return float(self.y[top].mean())\n",
    "        w = w / w.sum()\n",
    "        return float(w @ self.y[top])\n",
    "    def predict(self, m):\n",
    "        return [self.predict_one(r) for r in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ec9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph2VecHybrid:\n",
    "    def __init__(self, graph_weight=0.6, graph_dims=192, k=7, min_similarity=0.02, graph_seed=42):\n",
    "        self.gw = graph_weight\n",
    "        self.seed = graph_seed\n",
    "        self.text = TextFeatures()\n",
    "        self.graph = Graph2VecEmbedder(dimensions=graph_dims, seed=graph_seed)\n",
    "        self.knn = WeightedKNN(k, min_similarity)\n",
    "    def _l2(self, m: np.ndarray) -> np.ndarray:\n",
    "        m = np.atleast_2d(m)\n",
    "        n = np.linalg.norm(m, axis=1, keepdims=True)\n",
    "        n[n == 0] = 1\n",
    "        return m / n\n",
    "    def _align(self, g, t):\n",
    "        if g.shape[1] == t.shape[1]:\n",
    "            return g, t\n",
    "        d = max(g.shape[1], t.shape[1])\n",
    "        if g.shape[1] < d:\n",
    "            g = np.pad(g, ((0, 0), (0, d - g.shape[1])))\n",
    "        if t.shape[1] < d:\n",
    "            t = np.pad(t, ((0, 0), (0, d - t.shape[1])))\n",
    "        return g, t\n",
    "    def _blend(self, g, t):\n",
    "        g, t = self._align(self._l2(g), self._l2(t))\n",
    "        return self.gw * g + (1 - self.gw) * t\n",
    "    def fit(self, erds: Sequence[Dict], labels: Sequence[float]):\n",
    "        self.train_erds = list(erds)\n",
    "        self.train_labels = np.array(labels, float)\n",
    "        self.text.fit(erds)\n",
    "        self.train_text = self.text.transform(erds)\n",
    "        g = self.graph.fit(erds)\n",
    "        self.knn.fit(self._blend(g, self.train_text), labels)\n",
    "    def predict(self, erds: Sequence[Dict]) -> List[float]:\n",
    "        if not erds:\n",
    "            return []\n",
    "        new_text = self.text.transform(erds)\n",
    "        tg, eg = self.graph.fit_transform_pair(self.train_erds, erds)\n",
    "        blend_train = self._blend(tg, self.train_text)\n",
    "        blend_new = self._blend(eg, new_text)\n",
    "        self.knn.fit(blend_train, self.train_labels)\n",
    "        return self.knn.predict(blend_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3514b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage2_rmse(y_true, y_pred, drop=0.15):\n",
    "    yt = np.array(y_true, float)\n",
    "    yp = np.array(y_pred, float)\n",
    "    if yt.size == 0:\n",
    "        return float(\"nan\")\n",
    "    err = np.abs(yt - yp)\n",
    "    keep = max(1, int(np.ceil(err.size * (1 - drop))))\n",
    "    idx = np.argsort(err)[:keep]\n",
    "    return float(math.sqrt(np.mean((err[idx]) ** 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa707b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAG_SEEDS = [17, 42, 123]\n",
    "def bagged_predict(train_e, train_y, eval_e, params, seeds=BAG_SEEDS):\n",
    "    preds = []\n",
    "    for s in seeds:\n",
    "        m = Graph2VecHybrid(graph_seed=s, **params)\n",
    "        m.fit(train_e, train_y)\n",
    "        preds.append(np.array(m.predict(eval_e), float))\n",
    "    return np.mean(preds, axis=0).tolist() if preds else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85519405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 graded ERDs for training and 62 for testing (for_testing folder).\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"/Users/neelvachhani/Downloads/for_students\")\n",
    "GRADES_CSV = DATA_ROOT/\"ERD_grades.csv\"\n",
    "all_erds = load_all_erds(DATA_ROOT)\n",
    "grades = load_grades(GRADES_CSV)\n",
    "train_erds, train_labels = build_train_lists(all_erds, grades)\n",
    "test_erds = load_for_testing(DATA_ROOT)\n",
    "print(f\"Loaded {len(train_erds)} graded ERDs for training and {len(test_erds)} for testing (for_testing folder).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137b200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fixed params {'graph_weight': 0.4, 'graph_dims': 224, 'k': 11, 'min_similarity': 0.01} with seeds [17, 42, 123] on 62 ERDs\n"
     ]
    }
   ],
   "source": [
    "PARAMS = {\"graph_weight\":0.4,\"graph_dims\":224,\"k\":11,\"min_similarity\":0.01}\n",
    "SEEDS = [17,42,123]\n",
    "train_e = train_erds\n",
    "train_y = train_labels\n",
    "export_erds = test_erds if test_erds else list(all_erds.values())\n",
    "preds = bagged_predict(train_e, train_y, export_erds, PARAMS, SEEDS)\n",
    "print(f\"Ran fixed params {PARAMS} with seeds {SEEDS} on {len(export_erds)} ERDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75cf25c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/neelvachhani/Downloads/for_students/a3_graph2vec.csv (params={'graph_weight': 0.4, 'graph_dims': 224, 'k': 11, 'min_similarity': 0.01}, seeds=[17, 42, 123])\n"
     ]
    }
   ],
   "source": [
    "def _write_csv(preds_arr, erds_arr, sub_path):\n",
    "    sub_rows = {}\n",
    "    for e in erds_arr:\n",
    "        sub_rows.setdefault(int(e[\"ds\"][0]), [None, None])\n",
    "    for e, p in zip(erds_arr, preds_arr):\n",
    "        row = sub_rows[int(e[\"ds\"][0])]\n",
    "        row[e[\"ds\"][1]-1] = round(float(p), 2)\n",
    "    pd.DataFrame([{ \"ERD_No\": erd, \"dataset1_grade\": vals[0], \"dataset2_grade\": vals[1]} for erd, vals in sorted(sub_rows.items())]).to_csv(sub_path, index=False)\n",
    "sub_alias = DATA_ROOT/\"a3_graph2vec.csv\"\n",
    "_write_csv(preds, export_erds, sub_alias)\n",
    "print(f\"Wrote {sub_alias} (params={PARAMS}, seeds={SEEDS})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
