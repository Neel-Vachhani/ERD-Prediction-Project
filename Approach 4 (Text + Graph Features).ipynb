{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795e4067",
   "metadata": {},
   "source": [
    "# Entity similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "115828ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2) -> float:\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return (intersection + 1)/ (union + 1)\n",
    "\n",
    "def attribute_similarity_score(attrs1, attrs2) -> float:\n",
    "    set1 = set(get_lower_case_list(attrs1))\n",
    "    set2 = set(get_lower_case_list(attrs2))\n",
    "    return jaccard_similarity(set1, set2)\n",
    "\n",
    "def get_lower_case_list(values):\n",
    "    if type(values) is list:\n",
    "        lowered = []\n",
    "        for value in values:\n",
    "            if type(value) is str:\n",
    "                lowered.append(value.lower())\n",
    "            elif type(value) is list:\n",
    "                lowered.extend(get_lower_case_list(value))\n",
    "        return lowered\n",
    "    else:\n",
    "        return values.lower().split()\n",
    "\n",
    "def get_name(entity):\n",
    "    val = entity.get('name')\n",
    "    if isinstance(val, list):\n",
    "        return ' '.join(val)\n",
    "    return str(val)\n",
    "\n",
    "parameters = {\n",
    "    \"A\": 0.6,              # weight for type similarity\n",
    "    \"B\": 0.05,             # weight for name similarity\n",
    "    \"C\": 0.1,              # weight for attribute number similarity\n",
    "    \"D\": 0.25,             # weight for attribute similarity\n",
    "    \"DIFFERENT_TYPE\": 0.5, # penalty for different types\n",
    "    \"ALL_PAIRS\": True     # set to True for option 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "211326fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# load the pre-trained SBERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c20380af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_similarity(name1, name2) -> float:\n",
    "    #TODO: implement name similarity using SBERT embeddings and cosine similarity\n",
    "    embedding1 = model.encode(make_list_into_string(name1))\n",
    "    embedding2 = model.encode(make_list_into_string(name2))\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "    #print(f\"Name1: {make_list_into_string(name1)}, Name2: {make_list_into_string(name2)}, Similarity: {similarity[0][0]}\")\n",
    "    return similarity[0][0]\n",
    "\n",
    "def make_list_into_string(value) -> str:\n",
    "    # replace '_' with space and join list elements if value is a list\n",
    "    if type(value) is list:\n",
    "        return ' '.join([str(v).replace('_', ' ') for v in value])\n",
    "    else:\n",
    "        return str(value).replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0987a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_similarity(e1, e2, parameters) -> float:\n",
    "    '''Compute similarity between two entities e1 and e2.'''\n",
    "    assert parameters['A'] + parameters['B'] + parameters['C'] + parameters['D'] == 1.0, \"Weights must sum to 1.0\"\n",
    "    \n",
    "    type_similarity = 1.0 if (e1['kind'] == e2['kind']) else parameters[\"DIFFERENT_TYPE\"]\n",
    "    e1_list = get_lower_case_list(e1['name'])\n",
    "    e2_list = get_lower_case_list(e2['name'])\n",
    "    #name_similarity = 0.5\n",
    "    #name_similarity = get_name_similarity(e1_list, e2_list)\n",
    "    name_similarity = jaccard_similarity(set(e1_list), set(e2_list))\n",
    "    \n",
    "    attribute_number_similarity = 1.0 - abs(len(e1['attributes']) - len(e2['attributes'])) / max(len(e1['attributes']), len(e2['attributes']), 1)\n",
    "    attribute_similarity = attribute_similarity_score(e1['attributes'], e2['attributes'])\n",
    "    overall_similarity = (parameters['A'] * type_similarity +\n",
    "                          parameters['B'] * name_similarity +\n",
    "                          parameters['C'] * attribute_number_similarity +\n",
    "                          parameters['D'] * attribute_similarity)\n",
    "    #print(overall_similarity)\n",
    "    return overall_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bc88ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_entities_similarity(erd1, erd2, parameters) -> tuple[float, dict]:\n",
    "    '''\n",
    "    Compute overall similarity between two ERD diagrams by comparing all entities.\n",
    "    Returns: (average_similarity_score, entity_matches_dictionary)\n",
    "    '''\n",
    "    similarities = []\n",
    "    for e1 in erd1['entities']:\n",
    "        for e2 in erd2['entities']:\n",
    "            sim = entity_similarity(e1, e2, parameters)\n",
    "            similarities.append((e1, e2, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    e1_paired = {}\n",
    "    e2_paired = {}\n",
    "    pairs = []\n",
    "\n",
    "    entity_matches = {}\n",
    "\n",
    "    for e1, e2, sim in similarities:\n",
    "        e1_name = get_name(e1)\n",
    "        e2_name = get_name(e2)\n",
    "        if e1_name not in e1_paired and e2_name not in e2_paired:\n",
    "            e1_paired[e1_name] = e1\n",
    "            e2_paired[e2_name] = e2\n",
    "            pairs.append((e1, e2, sim))\n",
    "            \n",
    "            entity_matches[e1_name] = (e2_name, sim)\n",
    "            \n",
    "\n",
    "    if not parameters[\"ALL_PAIRS\"]: \n",
    "        if not pairs: return 0.0, {}\n",
    "        average_similarity = sum([sim for e1, e2, sim in pairs]) / len(pairs)\n",
    "        return average_similarity, entity_matches\n",
    "\n",
    "    for e1 in erd1['entities']:\n",
    "        e1_name = get_name(e1)\n",
    "        if e1_name not in e1_paired:\n",
    "            pairs.append((e1, None, 0))\n",
    "    for e2 in erd2['entities']:\n",
    "        e2_name = get_name(e2)\n",
    "        if e2_name not in e2_paired:\n",
    "            pairs.append((None, e2, 0))\n",
    "            \n",
    "    if not pairs: return 0.0, {}\n",
    "    average_similarity = sum([sim for e1, e2, sim in pairs]) / len(pairs)\n",
    "    \n",
    "    return average_similarity, entity_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd9f44",
   "metadata": {},
   "source": [
    "# Relationship similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a40758ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_cardinality(card_str):\n",
    "    \"\"\"\n",
    "    Parses cardinality strings (e.g., '0M', '11') into Min and Max components.\n",
    "    Returns tuple (min_card, max_card).\n",
    "    'M' or 'N' is treated as 'Many'.\n",
    "    \"\"\"\n",
    "    if not card_str or str(card_str).lower() == 'unknown':\n",
    "        return None, None\n",
    "    \n",
    "    # Standardize 'N' to 'M' for comparison\n",
    "    clean_str = str(card_str).upper().replace('N', 'M')\n",
    "    \n",
    "    # Assume format is MinMax (e.g., '0M' -> Min 0, Max Many)\n",
    "    if len(clean_str) == 2:\n",
    "        return clean_str[0], clean_str[1]\n",
    "    return clean_str, clean_str\n",
    "\n",
    "def compute_cardinality_similarity(c1, c2):\n",
    "    \"\"\"Simple binary match for cardinality components.\"\"\"\n",
    "    if c1 is None or c2 is None:\n",
    "        return 0.0\n",
    "    return 1.0 if c1 == c2 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "44e24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Cell 23\n",
    "def relationship_similarity(r1, r2, entity_matches, parameters) -> float:\n",
    "    \"\"\"\n",
    "    Computes similarity between two relationships r1 and r2.\n",
    "    \"\"\"\n",
    "    # Helper to safe-string names that might be lists\n",
    "    def _safe_name(n):\n",
    "        if isinstance(n, list):\n",
    "            return ' '.join(n).lower()\n",
    "        return str(n).lower()\n",
    "\n",
    "    # 1. Type Similarity (Weight T)\n",
    "    type_sim = 1.0 if r1.get('kind') == r2.get('kind') else parameters.get('REL_DIFF_TYPE', 0.5)\n",
    "\n",
    "    # 2. Arity Similarity (Weight U)\n",
    "    len1 = len(r1['involved_entities'])\n",
    "    len2 = len(r2['involved_entities'])\n",
    "    arity_sim = 1.0 - abs(len1 - len2) / max(len1, len2, 1)\n",
    "\n",
    "    # 3. Participating Entities Similarity (Weight V)\n",
    "    matched_scores = []\n",
    "    \n",
    "    # Create a lookup for r2's involved entities using safe string names\n",
    "    r2_involved_names = {_safe_name(e['name']): e for e in r2['involved_entities']}\n",
    "    \n",
    "    # Lists to store aligned cardinalities for later steps\n",
    "    r1_cardinalities = []\n",
    "    r2_cardinalities = []\n",
    "\n",
    "    for ent1 in r1['involved_entities']:\n",
    "        ent1_name = get_name(ent1) # Use get_name to ensure we match the keys in entity_matches\n",
    "        \n",
    "        match_info = entity_matches.get(ent1_name) \n",
    "        \n",
    "        sim_score = 0.0\n",
    "        r2_card = None\n",
    "        \n",
    "        if match_info:\n",
    "            matched_ent2_name, match_score = match_info\n",
    "            # Check if this globally matched entity is actually involved in relationship r2\n",
    "            if matched_ent2_name and matched_ent2_name.lower() in r2_involved_names:\n",
    "                sim_score = match_score\n",
    "                r2_card = r2_involved_names[matched_ent2_name.lower()].get('cardinality')\n",
    "            else:\n",
    "                sim_score = 0.0\n",
    "        \n",
    "        matched_scores.append(sim_score)\n",
    "        r1_cardinalities.append(ent1.get('cardinality'))\n",
    "        r2_cardinalities.append(r2_card)\n",
    "\n",
    "    entity_sim = sum(matched_scores) / len(matched_scores) if matched_scores else 0.0\n",
    "\n",
    "    # 4. Attribute Similarity (Weight X)\n",
    "    # Using simple Jaccard fallback to avoid more errors, or insert embedding code here if desired\n",
    "    attr1 = set(get_lower_case_list(r1.get('attributes', [])))\n",
    "    attr2 = set(get_lower_case_list(r2.get('attributes', [])))\n",
    "    \n",
    "    if not attr1 and not attr2:\n",
    "        attr_sim = 1.0\n",
    "    else:\n",
    "        intersection = len(attr1 & attr2)\n",
    "        union = len(attr1 | attr2)\n",
    "        attr_sim = intersection / union if union > 0 else 0.0\n",
    "\n",
    "    # 5. Cardinality Similarity (Weight Y and Z)\n",
    "    max_card_scores = []\n",
    "    min_card_scores = []\n",
    "\n",
    "    for c1_str, c2_str in zip(r1_cardinalities, r2_cardinalities):\n",
    "        min1, max1 = parse_cardinality(c1_str)\n",
    "        min2, max2 = parse_cardinality(c2_str)\n",
    "        \n",
    "        max_card_scores.append(compute_cardinality_similarity(max1, max2))\n",
    "        min_card_scores.append(compute_cardinality_similarity(min1, min2))\n",
    "\n",
    "    max_card_sim = sum(max_card_scores) / len(max_card_scores) if max_card_scores else 0.0\n",
    "    min_card_sim = sum(min_card_scores) / len(min_card_scores) if min_card_scores else 0.0\n",
    "\n",
    "    # Weighted Sum\n",
    "    total_sim = (parameters['T'] * type_sim +\n",
    "                 parameters['U'] * arity_sim +\n",
    "                 parameters['V'] * entity_sim +\n",
    "                 parameters['X'] * attr_sim +\n",
    "                 parameters['Y'] * max_card_sim +\n",
    "                 parameters['Z'] * min_card_sim)\n",
    "    \n",
    "    return total_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f82983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def all_relationships_similarity(erd1, erd2, entity_matches, parameters) -> float:\n",
    "    \"\"\"\n",
    "    Computes overall similarity between two ERDs based on their relationships.\n",
    "    Matches relationships greedily similar to entities.\n",
    "    \"\"\"\n",
    "    rels1 = erd1.get('relationships', [])\n",
    "    rels2 = erd2.get('relationships', [])\n",
    "    \n",
    "    if not rels1 and not rels2:\n",
    "        return 1.0\n",
    "    if not rels1 or not rels2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate all pair similarities\n",
    "    all_pairs = []\n",
    "    for i, r1 in enumerate(rels1):\n",
    "        for j, r2 in enumerate(rels2):\n",
    "            sim = relationship_similarity(r1, r2, entity_matches, parameters)\n",
    "            all_pairs.append({'r1_idx': i, 'r2_idx': j, 'sim': sim})\n",
    "\n",
    "    # Sort by similarity descending (Greedy Match)\n",
    "    all_pairs.sort(key=lambda x: x['sim'], reverse=True)\n",
    "\n",
    "    matched_r1 = set()\n",
    "    matched_r2 = set()\n",
    "    final_matches = []\n",
    "\n",
    "    for pair in all_pairs:\n",
    "        if pair['r1_idx'] not in matched_r1 and pair['r2_idx'] not in matched_r2:\n",
    "            matched_r1.add(pair['r1_idx'])\n",
    "            matched_r2.add(pair['r2_idx'])\n",
    "            final_matches.append(pair['sim'])\n",
    "\n",
    "    # Handle Unmatched Relationships (Option 1 vs Option 2)\n",
    "    # Project Requirement: Option 1 averages all, Option 2 excludes NULLs[cite: 96].\n",
    "    \n",
    "    if parameters.get('REL_ALL_PAIRS', True): # Option 1 (Default)\n",
    "        # Add 0s for unmatched relationships from r1\n",
    "        unmatched_count_1 = len(rels1) - len(matched_r1)\n",
    "        final_matches.extend([0.0] * unmatched_count_1)\n",
    "        \n",
    "        # Add 0s for unmatched relationships from r2\n",
    "        unmatched_count_2 = len(rels2) - len(matched_r2)\n",
    "        final_matches.extend([0.0] * unmatched_count_2)\n",
    "\n",
    "    return sum(final_matches) / len(final_matches) if final_matches else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "09d2defd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters defined successfully: {'A': 0.6, 'B': 0.05, 'C': 0.1, 'D': 0.25, 'DIFFERENT_TYPE': 0.5, 'ALL_PAIRS': True, 'T': 0.1, 'U': 0.3, 'V': 0.4, 'X': 0.1, 'Y': 0.05, 'Z': 0.05, 'REL_DIFF_TYPE': 0.1, 'REL_ALL_PAIRS': True}\n"
     ]
    }
   ],
   "source": [
    "# FOR SETUP AND TESTING\n",
    "\n",
    "parameters.update({\n",
    "    \"T\": 0.1,  # Relationship: Type weight\n",
    "    \"U\": 0.3,  # Relationship: Arity weight \n",
    "    \"V\": 0.4,  # Relationship: Entity participation weight\n",
    "    \"X\": 0.1,  # Relationship: Attribute weight\n",
    "    \"Y\": 0.05, # Relationship: Max Cardinality weight\n",
    "    \"Z\": 0.05, # Relationship: Min Cardinality weight\n",
    "    \"REL_DIFF_TYPE\": 0.1, # Penalty for different relationship types\n",
    "    \"REL_ALL_PAIRS\": True # True = Option 1 (Average all), False = Option 2 (Exclude NULLs)\n",
    "})\n",
    "\n",
    "# Example Usage for Testing\n",
    "# You must provide the 'entity_matches' from Step 2.\n",
    "# Structure: {'EntityIn1': ('BestMatchIn2', SimilarityScore)}\n",
    "example_matches = {\n",
    "    'student': ('pupil', 0.9), \n",
    "    'course': ('class', 0.85)\n",
    "}\n",
    "\n",
    "print(\"Parameters defined successfully:\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b147e3e",
   "metadata": {},
   "source": [
    "# Other similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "629cbf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_entities(erd):\n",
    "    return len([entity for entity in erd['entities'] if entity['kind'] == 'entity'])\n",
    "def number_of_weak_entities(erd):\n",
    "    return len([entity for entity in erd['entities'] if entity['kind'] == 'weak_entity'])\n",
    "def avg_number_of_relationship_attributes(erd):\n",
    "    attribute_counts = [len(relationship['attributes']) for relationship in erd['relationships']]\n",
    "    return sum(attribute_counts) / len(attribute_counts)\n",
    "def num_binary_relationships(erd):\n",
    "    return len([relationship for relationship in erd['relationships'] if relationship['kind'] == 'relationship' and len(relationship['involved_entities']) == 2])\n",
    "def num_binary_identifying_relationships(erd):\n",
    "    return len([relationship for relationship in erd['relationships'] if relationship['kind'] == 'identifying_relationship' and len(relationship['involved_entities']) == 2])\n",
    "def num_ternary_relationships(erd):\n",
    "    return len([relationship for relationship in erd['relationships'] if relationship['kind'] == 'relationship' and len(relationship['involved_entities']) == 3])\n",
    "def num_ternary_identifying_relationships(erd):\n",
    "    return len([relationship for relationship in erd['relationships'] if relationship['kind'] == 'identifying_relationship' and len(relationship['involved_entities']) == 3])\n",
    "def num_other_relationships(erd):\n",
    "    return len([[relationship for relationship in erd['relationships'] if relationship['kind'] == 'relationship' and len(relationship['involved_entities']) > 3]])\n",
    "def num_other_identifying_relationships(erd):\n",
    "    return len([relationship for relationship in erd['relationships'] if relationship['kind'] == 'identifying_relationship' and len(relationship['involved_entities']) > 3])\n",
    "\n",
    "def other_similarity(erd1, erd2) -> float:\n",
    "    weights = [1, 1, 1, 1, 1]\n",
    "    applier = [\n",
    "        number_of_entities,\n",
    "        number_of_weak_entities,\n",
    "        avg_number_of_relationship_attributes,\n",
    "        num_binary_relationships,\n",
    "        num_binary_identifying_relationships,\n",
    "        num_ternary_relationships,\n",
    "        num_ternary_identifying_relationships,\n",
    "        num_other_relationships,\n",
    "        num_other_identifying_relationships\n",
    "    ]\n",
    "    erd1_vector = [a(erd1) for a in applier]\n",
    "    erd2_vector = [a(erd2) for a in applier]\n",
    "    for i in range(len(weights)):\n",
    "        erd1_vector[i] *= weights[i]\n",
    "        erd2_vector[i] *= weights[i]\n",
    "    dotted = sum(v1 * v2 for v1, v2 in zip(erd1_vector, erd2_vector))\n",
    "    m1 = sum(v1 * v1 for v1, v1 in zip(erd1_vector, erd1_vector))**0.5\n",
    "    m2 = sum(v2 * v2 for v2, v2 in zip(erd1_vector, erd1_vector))**0.5\n",
    "    return dotted / m1 / m2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50905c",
   "metadata": {},
   "source": [
    "# Tying it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2db838f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def build_train_test_sets(root_dir: str = \".\",\n",
    "                          dataset1_dir: str = \"Dataset1\",\n",
    "                          dataset2_dir: str = \"for_testing/Dataset1\") -> Tuple[Dict[str, dict], Dict[str, dict]]:\n",
    "    \"\"\"\n",
    "    Load JSON files from Dataset1 (train) and Dataset2 (test) subfolders of `root_dir`.\n",
    "    Returns (train_set, test_set) where each is a dict: {ERD_number_as_str: parsed_json_object}.\n",
    "    \"\"\"\n",
    "    def load_dir(dir_path: str) -> Dict[str, dict]:\n",
    "        out: Dict[str, dict] = {}\n",
    "        if not os.path.isdir(dir_path):\n",
    "            return out\n",
    "        for fname in sorted(os.listdir(dir_path)):\n",
    "            if not fname.lower().endswith(\".json\"):\n",
    "                continue\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            erd_part = base.split(\"_\", 1)[0]\n",
    "            erd_key = str(erd_part)\n",
    "            file_path = os.path.join(dir_path, fname)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    obj = json.load(f)\n",
    "                out[erd_key] = obj\n",
    "            except json.JSONDecodeError as e:\n",
    "                # skip malformed JSON but you can log or collect errors if desired\n",
    "                print(f\"Warning: JSON decode error in {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to read {file_path}: {e}\")\n",
    "        return out\n",
    "\n",
    "    d1 = os.path.join(root_dir, dataset1_dir)\n",
    "    d2 = os.path.join(root_dir, dataset2_dir)\n",
    "\n",
    "    train_set = load_dir(d1)\n",
    "    test_set = load_dir(d2)\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e3339f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Iterable, Optional, Any\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "_stemmer = PorterStemmer()\n",
    "\n",
    "def _split_camel_case(s: str) -> str:\n",
    "    s = re.sub(r'([a-z0-9])([A-Z])', r'\\1 \\2', s)\n",
    "    s = re.sub(r'([A-Z])([A-Z][a-z])', r'\\1 \\2', s)\n",
    "    return s\n",
    "\n",
    "def _normalize_string_to_tokens(s: str, use_stemming: bool = True) -> list:\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = _split_camel_case(s)\n",
    "    s = s.replace('-', ' ').replace('_', ' ').replace('/', ' ').replace('\\\\', ' ')\n",
    "    s = s.lower()\n",
    "    # keep letters and spaces only (follow Stage_2_Approach1 normalization)\n",
    "    s = re.sub(r'[^a-z\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    # safe whole-word replacements\n",
    "    s = re.sub(r'\\bnum\\b', 'number', s)\n",
    "    s = re.sub(r'\\bid\\b', 'identifier', s)\n",
    "    s = re.sub(r'\\bno\\b', 'number', s)\n",
    "    tokens = [t for t in s.split() if t]\n",
    "    if use_stemming and tokens:\n",
    "        tokens = [_stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def normalize_erds(json_map: Dict[str, dict],\n",
    "                   keys_to_normalize: Optional[Iterable[str]] = None,\n",
    "                   use_stemming: bool = True,\n",
    "                   as_tokens: bool = False) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Normalize ERD JSON objects in-place.\n",
    "\n",
    "    Args:\n",
    "      json_map: mapping ERD_key (string) -> parsed JSON object (dict).\n",
    "      keys_to_normalize: iterable of key names to normalize (defaults to ('name','primary_keys','attributes')).\n",
    "      use_stemming: whether to apply Porter stemming (default True).\n",
    "      as_tokens: if True store lists of tokens; if False store normalized strings (joined tokens).\n",
    "\n",
    "    Behavior:\n",
    "      - Recurses each ERD dict and normalizes any value for keys in `keys_to_normalize`.\n",
    "      - If a value is a string -> replaced with normalized string (or list of tokens if as_tokens=True).\n",
    "      - If a value is a list of strings -> each string is normalized.\n",
    "      - Does NOT modify other keys (e.g., 'kind' or 'cardinality').\n",
    "\n",
    "    Returns:\n",
    "      The same `json_map` (modified in-place).\n",
    "    \"\"\"\n",
    "    if keys_to_normalize is None:\n",
    "        keys_to_normalize = ('name', 'primary_keys', 'attributes')\n",
    "    keys_set = set(keys_to_normalize)\n",
    "\n",
    "    def _process_value(val: Any) -> Any:\n",
    "        if isinstance(val, str):\n",
    "            toks = _normalize_string_to_tokens(val, use_stemming=use_stemming)\n",
    "            return toks if as_tokens else \" \".join(toks)\n",
    "        if isinstance(val, list):\n",
    "            # list of strings -> normalize each string\n",
    "            if all(isinstance(x, str) for x in val):\n",
    "                out = []\n",
    "                for x in val:\n",
    "                    toks = _normalize_string_to_tokens(x, use_stemming=use_stemming)\n",
    "                    out.append(toks if as_tokens else \" \".join(toks))\n",
    "                return out\n",
    "            # heterogeneous list -> recurse elements (e.g. list of dicts)\n",
    "            return [_recurse(x) for x in val]\n",
    "        # leave other types unchanged\n",
    "        return val\n",
    "\n",
    "    def _recurse(obj: Any) -> Any:\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in list(obj.items()):\n",
    "                if k in keys_set:\n",
    "                    # only normalize the specified keys\n",
    "                    obj[k] = _process_value(v)\n",
    "                else:\n",
    "                    # recurse into nested structures to find target keys (e.g., involved_entities -> dicts with 'name')\n",
    "                    if isinstance(v, (dict, list)):\n",
    "                        obj[k] = _recurse(v)\n",
    "            return obj\n",
    "        if isinstance(obj, list):\n",
    "            return [_recurse(x) for x in obj]\n",
    "        return obj\n",
    "\n",
    "    for key, erd in json_map.items():\n",
    "        json_map[key] = _recurse(erd)\n",
    "\n",
    "    return json_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6b067f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ESTIMATES GRADES GIVEN:\n",
    "### training set\n",
    "### training set's grades\n",
    "### test set\n",
    "### k for neighbors, similarity threshold\n",
    "### alpha, beta, and gamma\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def knn_grade_estimates(\n",
    "    train_set: Dict[str, dict],\n",
    "    test_set: Dict[str, dict],\n",
    "    grade_map: Dict[str, float],\n",
    "    k: int = 4,\n",
    "    sigma: float = 0.9,\n",
    "    alpha: float = 0.4,\n",
    "    beta: float = 0.4,\n",
    "    gamma: float = 0.2,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    For each ERD in test_set, find k nearest neighbors among train_set with similarity >= sigma.\n",
    "    Similarity = alpha * ent_sim + beta * rel_sim + gamma * oth_sim.\n",
    "    \"\"\"\n",
    "    # Compute mean grade of dataset1 ERDs (only for keys present in train_set and in grade_map)\n",
    "    train_keys_with_grade = [k for k in train_set.keys() if k in grade_map and grade_map[k] is not None]\n",
    "    if train_keys_with_grade:\n",
    "        mean_grade = sum(grade_map[k] for k in train_keys_with_grade) / len(train_keys_with_grade)\n",
    "    else:\n",
    "        mean_grade = 0.0\n",
    "\n",
    "    predictions: Dict[str, float] = {}\n",
    "\n",
    "    for test_key, test_erd in test_set.items():\n",
    "        sims = []  # list of (similarity, train_key)\n",
    "\n",
    "        for train_key, train_erd in train_set.items():\n",
    "            # skip train ERDs without a dataset1 grade\n",
    "            if train_key not in grade_map or grade_map[train_key] is None:\n",
    "                continue\n",
    "\n",
    "            # 1. Calculate Entity Similarity AND get the match dictionary\n",
    "            ent_sim, entity_matches = all_entities_similarity(train_erd, test_erd, parameters)\n",
    "\n",
    "            # 2. Calculate Relationship Similarity using those matches\n",
    "            rel_sim = all_relationships_similarity(train_erd, test_erd, entity_matches, parameters)\n",
    "\n",
    "            # 3. Calculate Other Similarity\n",
    "            oth_sim = other_similarity(train_erd, test_erd)\n",
    "\n",
    "            # Weighted Sum\n",
    "            sim = alpha * ent_sim + beta * rel_sim + gamma * oth_sim\n",
    "\n",
    "            if sim >= sigma:\n",
    "                sims.append((sim, train_key))\n",
    "\n",
    "        # If there are at least k neighbors, use top-k by similarity, otherwise fallback to mean grade\n",
    "        if len(sims) >= k:\n",
    "            sims.sort(key=lambda x: x[0], reverse=True)\n",
    "            topk = sims[:k]\n",
    "            neighbor_grades = [grade_map[tk] for _, tk in topk]\n",
    "            pred = sum(neighbor_grades) / len(neighbor_grades)\n",
    "        else:\n",
    "            pred = mean_grade\n",
    "        predictions[test_key] = pred #+ round(random.uniform(0, 2), 4)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b07845bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import Dict, Optional\n",
    "\n",
    "### Returns a dictionary of erd_number : grade (dataset1)\n",
    "\n",
    "def build_grade_map(csv_path: str = \"ERD_grades.csv\", dataset: int = 1) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Read ERD grades from CSV and return a map {ERD_number_str: grade_or_None}.\n",
    "\n",
    "    Args:\n",
    "      csv_path: path to CSV file (default \"ERD_grades.csv\" in repo root).\n",
    "      dataset: which dataset column to use (1 -> \"dataset1_grade\", 2 -> \"dataset2_grade\").\n",
    "\n",
    "    Returns:\n",
    "      Dict where keys are ERD numbers as strings and values are floats or None (if missing/invalid).\n",
    "    \"\"\"\n",
    "    grade_col = f\"dataset{dataset}_grade\"\n",
    "    out: Dict[str, Optional[float]] = {}\n",
    "    try:\n",
    "        with open(csv_path, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "            reader = csv.DictReader(fh)\n",
    "            for row in reader:\n",
    "                # tolerate different header casing/whitespace\n",
    "                erd_key_raw = row.get(\"ERD_No\") or row.get(\"ERD_No\".lower()) or row.get(\"ERD_No\".upper()) or row.get(\"ERD No\") or row.get(\"ERD No\".lower())\n",
    "                if erd_key_raw is None:\n",
    "                    # try to find a column that looks like ERD number\n",
    "                    for k in row.keys():\n",
    "                        if k.strip().lower().replace(\" \", \"_\") in (\"erd_no\", \"erdno\", \"erd\"):\n",
    "                            erd_key_raw = row[k]\n",
    "                            break\n",
    "                if erd_key_raw is None:\n",
    "                    continue\n",
    "                erd_key = str(erd_key_raw).strip()\n",
    "                if not erd_key:\n",
    "                    continue\n",
    "\n",
    "                # find the grade column with tolerant matching\n",
    "                grade_val = None\n",
    "                if grade_col in row:\n",
    "                    grade_val = row[grade_col]\n",
    "                else:\n",
    "                    # try tolerant lookup\n",
    "                    for k in row.keys():\n",
    "                        if k.strip().lower() == grade_col.lower():\n",
    "                            grade_val = row[k]\n",
    "                            break\n",
    "\n",
    "                if grade_val is None:\n",
    "                    out[erd_key] = None\n",
    "                    continue\n",
    "\n",
    "                grade_val = grade_val.strip()\n",
    "                if grade_val == \"\":\n",
    "                    out[erd_key] = None\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    out[erd_key] = float(grade_val)\n",
    "                except ValueError:\n",
    "                    # if parsing fails, store None so callers can detect missing values\n",
    "                    out[erd_key] = None\n",
    "    except FileNotFoundError:\n",
    "        # file missing -> return empty map\n",
    "        print(f\"Warning: CSV file {csv_path} not found.\")\n",
    "        return {}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb37b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Optional\n",
    "\n",
    "### CALCULATE MEAN SQUARED ERROR\n",
    "\n",
    "def compute_rmse(predictions: Dict[str, float],\n",
    "                 csv_path: str = \"ERD_grades.csv\",\n",
    "                 dataset: int = 2) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute RMSE between `predictions` and actual grades from `csv_path`.\n",
    "    - predictions: {ERD_number_str: predicted_grade}\n",
    "    - dataset: which dataset column to compare against (1 -> dataset1_grade, 2 -> dataset2_grade)\n",
    "\n",
    "    Returns:\n",
    "      RMSE as float, or None if there are no matching actual grades to compare.\n",
    "    \"\"\"\n",
    "    actual_map = build_grade_map(csv_path=csv_path, dataset=dataset)\n",
    "    sq_sum = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for erd_key, pred in predictions.items():\n",
    "        if pred is None:\n",
    "            continue\n",
    "        actual = actual_map.get(erd_key)\n",
    "        if actual is None:\n",
    "            continue\n",
    "        try:\n",
    "            err = float(pred) - float(actual)\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "        sq_sum += err * err\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return None\n",
    "\n",
    "    mse = sq_sum / count\n",
    "    return math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eb7ce236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 100\n",
      "Test count: 30\n",
      "{'104': 80.0, '105': 76.3325, '106': 76.0, '107': 79.0, '108': 79.0, '109': 81.6675, '110': 72.33500000000001, '111': 77.6675, '112': 75.3325, '114': 74.6675, '115': 75.6675, '116': 76.3325, '117': 76.3325, '118': 71.6675, '119': 78.6675, '121': 79.6675, '122': 74.3325, '123': 74.0, '124': 74.3325, '125': 71.6675, '127': 71.6675, '128': 77.6675, '129': 79.0, '130': 78.66499999999999, '131': 77.0, '132': 72.33500000000001, '133': 77.33500000000001, '134': 76.6675, '135': 75.6675, '136': 75.3325}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "root = \".\"\n",
    "train_set, test_set = build_train_test_sets(root)\n",
    "\n",
    "print(\"Train count:\", len(train_set))\n",
    "print(\"Test count:\", len(test_set))\n",
    "\n",
    "\n",
    "# assuming train_set, test_set were built with build_train_test_sets and contain parsed JSON dicts\n",
    "normalize_erds(train_set)               # default: join tokens into normalized strings\n",
    "normalize_erds(test_set, as_tokens=True) # store token lists for test_set\n",
    "\n",
    "grade_map = build_grade_map()\n",
    "\n",
    "grade_estimates = knn_grade_estimates(\n",
    "    train_set, test_set, grade_map,\n",
    "    k=4, sigma=0.5,\n",
    "    alpha=0.3,beta=0.2, gamma=0.5\n",
    ")\n",
    "\n",
    "print(grade_estimates)\n",
    "print(compute_rmse(grade_estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4ff2ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 101\n",
      "Test count: 29\n",
      "{'104': 87.0, '105': 83.0, '106': 83.0, '107': 83.0, '108': 89.0, '109': 81.0, '110': 78.0, '111': 90.0, '112': 90.0, '114': 78.0, '115': 86.0, '116': 83.0, '117': 88.0, '118': 90.0, '119': 86.0, '121': 84.0, '122': 90.0, '123': 87.0, '124': 84.0, '125': 95.0, '128': 83.0, '129': 90.0, '130': 83.0, '131': 83.0, '132': 83.0, '133': 91.0, '134': 81.0, '135': 85.0, '136': 89.0}\n",
      "None\n",
      "{'104': [80.0, 87.0], '105': [76.3325, 83.0], '106': [76.0, 83.0], '107': [79.0, 83.0], '108': [79.0, 89.0], '109': [81.6675, 81.0], '110': [72.33500000000001, 78.0], '111': [77.6675, 90.0], '112': [75.3325, 90.0], '114': [74.6675, 78.0], '115': [75.6675, 86.0], '116': [76.3325, 83.0], '117': [76.3325, 88.0], '118': [71.6675, 90.0], '119': [78.6675, 86.0], '121': [79.6675, 84.0], '122': [74.3325, 90.0], '123': [74.0, 87.0], '124': [74.3325, 84.0], '125': [71.6675, 95.0], '127': [71.6675, None], '128': [77.6675, 83.0], '129': [79.0, 90.0], '130': [78.66499999999999, 83.0], '131': [77.0, 83.0], '132': [72.33500000000001, 83.0], '133': [77.33500000000001, 91.0], '134': [76.6675, 81.0], '135': [75.6675, 85.0], '136': [75.3325, 89.0]}\n"
     ]
    }
   ],
   "source": [
    "root = \".\"\n",
    "train_set, test_set = build_train_test_sets(root, dataset1_dir=\"Dataset2\", dataset2_dir=\"for_testing/Dataset2\")\n",
    "\n",
    "print(\"Train count:\", len(train_set))\n",
    "print(\"Test count:\", len(test_set))\n",
    "\n",
    "\n",
    "# assuming train_set, test_set were built with build_train_test_sets and contain parsed JSON dicts\n",
    "normalize_erds(train_set)               # default: join tokens into normalized strings\n",
    "normalize_erds(test_set, as_tokens=True) # store token lists for test_set\n",
    "\n",
    "grade_map_2 = build_grade_map(dataset=2)\n",
    "\n",
    "grade_estimates_2 = knn_grade_estimates(\n",
    "    train_set, test_set, grade_map_2,\n",
    "    k=4, sigma=0.5,\n",
    "    alpha=0.3,beta=0.2, gamma=0.5\n",
    ")\n",
    "\n",
    "print(grade_estimates_2)\n",
    "print(compute_rmse(grade_estimates_2, dataset=2))\n",
    "\n",
    "for erd_key, est_grade in grade_estimates.items():\n",
    "    alt_grade = grade_estimates_2.get(erd_key)\n",
    "    grade_estimates[erd_key] = [est_grade, alt_grade]\n",
    "    \n",
    "# i have modified grade_estimates to have [dataset1_grade dataset2]\n",
    "\n",
    "print(grade_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9b8903b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade estimates written to a4_custom_graph.csv\n"
     ]
    }
   ],
   "source": [
    "# take the dict grade_estimates and write to CSV\n",
    "import csv\n",
    "def write_grade_estimates_to_csv(grade_estimates: Dict[str, float], output_csv_path: str = \"a4_custom_graph.csv\"):\n",
    "    \"\"\"\n",
    "    Write the grade estimates to a CSV file with columns: ERD_No, Estimated_Grade.\n",
    "    \"\"\"\n",
    "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['ERD_No', 'dataset1_grade', 'dataset2_grade']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for erd_no, estimated_grade in grade_estimates.items():\n",
    "            writer.writerow({'ERD_No': erd_no, 'dataset1_grade': estimated_grade[0], 'dataset2_grade': estimated_grade[1]})\n",
    "    print(f\"Grade estimates written to {output_csv_path}\")\n",
    "write_grade_estimates_to_csv(grade_estimates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
